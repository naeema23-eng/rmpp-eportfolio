<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Reflections – Research Methods and Professional Practice e-Portfolio</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="main-wrapper">

    <header class="site-header">
      <div class="brand">
        <h1>Research Methods and Professional Practice</h1>
        <span>MSc Artificial Intelligence – e-Portfolio</span>
      </div>

      <nav class="main-nav">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="discussion.html">Discussions</a></li>
          <li><a href="research.html">Research &amp; Proposal</a></li>
          <li><a href="statistics.html">Statistics &amp; Data</a></li>
          <li><a href="reflections.html" class="active">Reflections</a></li>
          <li><a href="activities.html">Activities</a></li>
          <li><a href="about.html">About</a></li>
        </ul>
      </nav>
    </header>

    <main class="page-grid">
      <section>

        <!-- Reflective Activity 1 -->
        <div class="card">
          <div class="card-header">
            <h2 class="page-title">Reflective Activity 1: Ethics in Computing</h2>
            <div class="meta small-muted">
              Unit 1 · Ethics in Computing in the Age of Generative AI · Word count: ~1,000
            </div>
          </div>

          <h3>Ethics in Computing in the Age of Generative AI</h3>

          <h4>What – Context and focus</h4>
          <p>
            Since late 2022, generative AI (GenAI) tools have moved from research labs into everyday life. They now write code, draft emails, summarise medical articles and generate realistic images. As Correa et al. (2023) show, this rapid adoption has triggered a wave of more than 200 AI ethics guidelines worldwide, but these documents differ in scope, terminology and legal force. Deckard (2023) adds that organisations face strong pressure to “move fast” with GenAI to gain competitive advantage, even when governance structures are still immature.
          </p>

          <p>
            For computing professionals, this raises a simple but difficult question: how can we use GenAI responsibly when global norms are fragmented and the technology is still evolving? In this reflection I draw on Correa et al. (2023), Deckard (2023), and professional codes such as ACM (2018) and BCS (2021, 2024) to develop my own view. I also connect the discussion to my background as a biomedical engineer and AI student who is particularly interested in healthcare and human physiology.
          </p>

          <h4>So what – Analysis and position</h4>

          <h5>1. Fragmented principles and the need for enforceable baselines</h5>
          <p>
            Correa et al. (2023) show that most AI ethics documents repeat similar high-level values – fairness, transparency, accountability, privacy, human oversight – but differ on how to implement or enforce them. Many guidelines are voluntary and lack mechanisms for monitoring or sanctions. I see a risk that GenAI becomes governed by “ethics washing”: beautiful principles with little impact on practice.
          </p>

          <p>
            From my perspective, a more convincing approach is layered. Global and regional frameworks (e.g. OECD principles, EU AI Act, GDPR) should define minimum, enforceable baselines for safety, transparency and data protection. Professional bodies such as ACM and BCS should translate these into expectations for individual conduct (competence, integrity, duty to the public). Finally, organisations must convert all of this into concrete policies, risk registers and technical controls inside their software development life cycle.
          </p>

          <h5>2. Risk-tiered governance rather than one-size-fits-all rules</h5>
          <p>
            Not every GenAI system has the same potential for harm. A brainstorming assistant for marketing copy is very different from a model that helps clinicians prioritise patients or predicts mental-health risk from text. I agree with Correa et al. (2023) that ethical guidance should be sensitive to context. In practice I support a risk-tiered model: low-risk applications may rely on lightweight safeguards, whereas high-risk uses require formal impact assessments, red-teaming, domain expert review, and ongoing monitoring.
          </p>

          <p>
            This approach matches my own experience in healthcare technology, where risk classification already determines the depth of testing and documentation. Extending this logic to GenAI would link ethical concerns to concrete engineering practices such as evaluation datasets, fail-safe design and incident reporting.
          </p>

          <h5>3. Data governance, privacy and security</h5>
          <p>
            Many GenAI harms begin in the data pipeline rather than in the model itself. Training on scraped data without clear consent or licensing raises questions under data protection and intellectual property law. Using patient information with public cloud models can directly violate confidentiality rules. As someone working in the health sector, I find this aspect particularly sensitive.
          </p>

          <p>
            I believe responsible GenAI requires strong data governance: clear legal bases for processing, purpose limitation, data minimisation, and robust security controls. Where models are trained or fine-tuned on personal data, organisations should conduct Data Protection Impact Assessments and ensure that outputs cannot easily re-identify individuals. This links directly to GDPR requirements and to ACM principles 1.2 (avoid harm) and 1.6 (respect privacy).
          </p>

          <h5>4. Professional competence and honest use of GenAI</h5>
          <p>
            Correa et al. (2023) highlight that many guidelines are aspirational, but Deckard (2023) stresses that successful GenAI adoption depends on real organisational capability. For individual computing professionals, competence now includes understanding GenAI’s limitations: hallucinations, bias, prompt injection, misuse of training data, and the need for human oversight.
          </p>

          <p>
            In my own studies I already rely on GenAI tools to brainstorm ideas and polish writing, but this activity made me think more critically about transparency and academic integrity. I must clearly distinguish between my own analysis and AI-generated suggestions, and always acknowledge sources correctly. Professionally, I see a similar responsibility: engineers should document how GenAI was used in design or documentation and avoid presenting AI output as unquestionable truth.
          </p>

          <h5>5. Human and physiological perspective</h5>
          <p>
            As a biomedical engineer, I cannot ignore the human body behind the data. Physiological signals, clinical notes and mental-health assessments are deeply personal. If GenAI is used to analyse such data – for example predicting deterioration in an ambulance or generating patient information leaflets – errors can harm both physical and emotional wellbeing.
          </p>

          <p>
            My long-term goal is to integrate AI with healthcare in ways that support clinicians rather than replace them. This reflection reinforced the importance of human-centred design: interfaces that are understandable to patients, explanations that match their emotional state, and workflows that keep clinicians in control. GenAI can be a powerful assistant, but only if we design around human limits, including cognitive overload and stress.
          </p>

          <p>
            <strong>My position:</strong> Generative AI should be governed through risk-tiered, enforceable frameworks that combine global principles, local law, professional codes and concrete engineering practices. Data governance and human-centred design are as important as model performance.
          </p>

          <h4>Now what – Recommended course of action and impacts</h4>

          <p>
            If I had to recommend a practical course of action, it would include:
          </p>

          <ul>
            <li>Adopting clear GenAI policies and risk registers</li>
            <li>Requiring impact assessments and red-team testing for higher-risk systems</li>
            <li>Mandating transparency via model cards and AI-generated content labels</li>
            <li>Strengthening professional education about AI risks and legal duties</li>
            <li>Involving diverse stakeholders, including patients and marginalised groups</li>
          </ul>

          <p>
            These steps support compliance with GDPR and other laws, reduce social harms, and uphold ACM/BCS duties regarding competence, transparency and fairness.
          </p>

          <h4>Conclusion</h4>

          <p>
            This activity expanded my view of AI ethics from “exciting new tools” to understanding the deep connections between GenAI, law, society and human bodies. It reinforced that computing professionals must examine who could be harmed, what data is used, and whether stressed clinicians or patients can understand the system.
          </p>

          <p>
            For me, that is what ethics in the age of generative AI truly means.
          </p>

          <h4>References</h4>

          <ul>
            <li>ACM (2018) ACM Code of Ethics and Professional Conduct.</li>
            <li>BCS (2021) Code of Conduct.</li>
            <li>BCS (2024) Code of Conduct.</li>
            <li>Correa et al. (2023) Worldwide AI ethics: review of 200 guidelines.</li>
            <li>Deckard (2023) Executive considerations for GenAI.</li>
            <li>GDPR (2016) Regulation (EU) 2016/679.</li>
          </ul>

        </div>


        <!-- Reflective Activity 2 -->
        <div class="card">
          <div class="card-header">
            <h2>Reflective Activity 2: Case Study – Inappropriate Use of Surveys</h2>
            <div class="meta small-muted">Unit 5 · Reserved</div>
          </div>

          <p>This section will be completed once Unit 5 activities are released.</p>
        </div>

        <!-- Reasoning Quiz -->
        <div class="card">
          <div class="card-header">
            <h2>Reasoning Quiz – Mini Reflection</h2>
            <div class="meta small-muted">Unit 1 · Reserved</div>
          </div>

          <p>A short reasoning reflection will be added later.</p>
        </div>

        <!-- Final Reflection -->
        <div class="card">
          <div class="card-header">
            <h2>Final 1,000-word Reflection (Unit 12)</h2>
            <div class="meta small-muted">Reserved</div>
          </div>

          <p>This area will contain the full end-of-module reflection.</p>
        </div>

      </section>

      <aside class="sidebar">
        <div class="card">
          <div class="card-header"><h3>Reflective Themes</h3></div>

          <ul class="clean">
            <li><span class="tag-pill">Ethics</span>Professional responsibility</li>
            <li><span class="tag-pill">Research</span>Developing academic practice</li>
            <li><span class="tag-pill">Growth</span>Tracking progress</li>
            <li><span class="tag-pill">Future</span>Applying learning to AI in healthcare</li>
          </ul>
        </div>
      </aside>

    </main>

    <footer class="site-footer">
      Reflections · Research Methods and Professional Practice – e-Portfolio
    </footer>

  </div>
</body>
</html>

