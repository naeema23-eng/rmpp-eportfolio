<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Reflections – Research Methods and Professional Practice e-Portfolio</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="main-wrapper">
    <header class="site-header">
      <div class="brand">
        <h1>Research Methods and Professional Practice</h1>
        <span>MSc Artificial Intelligence – e-Portfolio</span>
      </div>
      <nav class="main-nav">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="discussion.html">Discussions</a></li>
          <li><a href="research.html">Research &amp; Proposal</a></li>
          <li><a href="statistics.html">Statistics &amp; Data</a></li>
          <li><a href="reflections.html" class="active">Reflections</a></li>
          <li><a href="activities.html">Activities</a></li>
          <li><a href="about.html">About</a></li>
        </ul>
      </nav>
    </header>

    <main class="page-grid">
      <section>
        <!-- Reflective Activity 1 -->
        <div class="card">
          <div class="card-header">
            <h2 class="page-title">Reflective Activity 1: Ethics in Computing</h2>
            <div class="meta small-muted">
              Unit 1 · Ethics in Computing in the Age of Generative AI · Author: Naeema Abdalla Ahmed Alnaqbi · Word count: ~1,000
            </div>
          </div>

          <p><strong>Ethics in Computing in the Age of Generative AI</strong></p>

          <p class="section-label">What – Context and focus</p>
          <p>
            Since late 2022, generative AI (GenAI) tools have moved from research labs into everyday life.
            They now write code, draft emails, summarise medical articles and generate realistic images.
            As Correa et al. (2023) show, this rapid adoption has triggered a wave of more than 200 AI ethics
            guidelines worldwide, but these documents differ in scope, terminology and legal force. Deckard (2023)
            adds that organisations face strong pressure to “move fast” with GenAI to gain competitive
            advantage, even when governance structures are still immature.
          </p>
          <p>
            For computing professionals, this raises a simple but difficult question: how can we use GenAI
            responsibly when global norms are fragmented and the technology is still evolving? In this reflection I
            draw on Correa et al. (2023), Deckard (2023), and professional codes such as ACM (2018) and BCS (2021,
            2024) to develop my own view. I also connect the discussion to my background as a biomedical engineer and
            AI student who is particularly interested in healthcare and human physiology.
          </p>

          <p class="section-label">So what – Analysis and position</p>

          <p><strong>1. Fragmented principles and the need for enforceable baselines</strong></p>
          <p>
            Correa et al. (2023) show that most AI ethics documents repeat similar high-level values – fairness,
            transparency, accountability, privacy, human oversight – but differ on how to implement or enforce
            them. Many guidelines are voluntary and lack mechanisms for monitoring or sanctions. I see a risk that
            GenAI becomes governed by “ethics washing”: beautiful principles with little impact on practice.
          </p>
          <p>
            From my perspective, a more convincing approach is layered. Global and regional frameworks (e.g. OECD
            principles, EU AI Act, GDPR) should define minimum, enforceable baselines for safety, transparency and
            data protection. Professional bodies such as ACM and BCS should translate these into expectations for
            individual conduct (competence, integrity, duty to the public). Finally, organisations must convert all
            of this into concrete policies, risk registers and technical controls inside their software development
            life cycle.
          </p>

          <p><strong>2. Risk-tiered governance rather than one-size-fits-all rules</strong></p>
          <p>
            Not every GenAI system has the same potential for harm. A brainstorming assistant for marketing copy is
            very different from a model that helps clinicians prioritise patients or predicts mental-health risk from
            text. I agree with Correa et al. (2023) that ethical guidance should be sensitive to context. In practice
            I support a risk-tiered model: low-risk applications may rely on lightweight safeguards, whereas high-risk
            uses require formal impact assessments, red-teaming, domain expert review, and ongoing monitoring.
          </p>
          <p>
            This approach matches my own experience in healthcare technology, where risk classification already
            determines the depth of testing and documentation. Extending this logic to GenAI would link ethical
            concerns to concrete engineering practices such as evaluation datasets, fail-safe design and incident
            reporting.
          </p>

          <p><strong>3. Data governance, privacy and security</strong></p>
          <p>
            Many GenAI harms begin in the data pipeline rather than in the model itself. Training on scraped data
            without clear consent or licensing raises questions under data protection and intellectual property law.
            Using patient information with public cloud models can directly violate confidentiality rules. As someone
            working in the health sector, I find this aspect particularly sensitive.
          </p>
          <p>
            I believe responsible GenAI requires strong data governance: clear legal bases for processing, purpose
            limitation, data minimisation, and robust security controls. Where models are trained or fine-tuned on
            personal data, organisations should conduct Data Protection Impact Assessments and ensure that outputs
            cannot easily re-identify individuals. This links directly to GDPR requirements and to ACM principles 1.2
            (avoid harm) and 1.6 (respect privacy).
          </p>

          <p><strong>4. Professional competence and honest use of GenAI</strong></p>
          <p>
            Correa et al. (2023) highlight that many guidelines are aspirational, but Deckard (2023) stresses that
            successful GenAI adoption depends on real organisational capability. For individual computing
            professionals, competence now includes understanding GenAI’s limitations: hallucinations, bias,
            prompt injection, misuse of training data, and the need for human oversight.
          </p>
          <p>
            In my own studies I already rely on GenAI tools to brainstorm ideas and polish writing, but this activity
            made me think more critically about transparency and academic integrity. I must clearly distinguish
            between my own analysis and AI-generated suggestions, and always acknowledge sources correctly.
            Professionally, I see a similar responsibility: engineers should document how GenAI was used in design or
            documentation and avoid presenting AI output as unquestionable truth.
          </p>

          <p><strong>5. Human and physiological perspective</strong></p>
          <p>
            As a biomedical engineer, I cannot ignore the human body behind the data. Physiological signals, clinical
            notes and mental-health assessments are deeply personal. If GenAI is used to analyse such data – for
            example predicting deterioration in an ambulance or generating patient information leaflets – errors
            can harm both physical and emotional wellbeing.
          </p>
          <p>
            My long-term goal is to integrate AI with healthcare in ways that support clinicians rather than replace
            them. This reflection reinforced the importance of human-centred design: interfaces that are
            understandable to patients, explanations that match their emotional state, and workflows that keep
            clinicians in control. GenAI can be a powerful assistant, but only if we design around human limits,
            including cognitive overload and stress.
          </p>
          <p>
            <strong>My position:</strong> Generative AI should be governed through risk-tiered, enforceable frameworks
            that combine global principles, local law, professional codes and concrete engineering practices. Data
            governance and human-centred design are as important as model performance.
          </p>

          <p class="section-label">Now what – Recommended course of action and impacts</p>
          <p>
            If I had to recommend a practical course of action, it would include:
          </p>
          <ul>
            <li>
              Adopting clear GenAI policies and risk registers in organisations, linking each use case to purpose,
              lawful basis, data categories and risk tier.
            </li>
            <li>
              Requiring impact assessments and red-team testing for medium and high-risk applications, especially in
              health, education, employment and policing.
            </li>
            <li>
              Mandating transparency mechanisms such as model cards, change logs, and clear labelling of AI-generated
              content.
            </li>
            <li>
              Strengthening professional education so that computing practitioners understand GenAI failure modes,
              legal obligations and ethical reasoning, not just technical skills.
            </li>
            <li>
              Including diverse stakeholders – patients, disabled users, marginalised groups – in the design and
              evaluation of GenAI systems.
            </li>
          </ul>
          <p>
            Legally, these steps help organisations demonstrate compliance with data protection and consumer
            protection laws and with sector-specific regulations in healthcare. Socially, they can reduce bias,
            improve accessibility and preserve public trust. Professionally, they align with ACM and BCS duties to act
            in the public interest, maintain competence, and be honest about system limitations.
          </p>

          <p class="section-label">Conclusion</p>
          <p>
            This activity moved my thinking from “GenAI is exciting and powerful” to a more nuanced view:
            GenAI is also fragile, highly context-dependent and deeply entangled with law, society and human bodies.
            Reading Correa et al. (2023) made me appreciate how fragmented global AI governance currently is, while
            Deckard (2023) emphasised that uncritical adoption is strategically risky.
          </p>
          <p>
            As I continue my MSc and future work in AI-enabled healthcare, I want to be the kind of professional who
            can translate ethical principles into practical decisions: asking “who might be harmed?”,
            “what data are we really using?” and “how can we design this so a stressed clinician or
            patient can still understand it?”. For me, that is what ethics in the age of generative AI truly means.
          </p>

          <p class="section-label">References</p>
          <ul>
            <li>
              ACM (2018) <em>ACM Code of Ethics and Professional Conduct</em>. Available at:
              https://www.acm.org/code-of-ethics (Accessed: 12 November 2025).
            </li>
            <li>
              BCS (2021) <em>Code of Conduct</em>. British Computer Society. Available at:
              https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/ (Accessed: 12 November 2025).
            </li>
            <li>
              BCS (2024) <em>BCS Code of Conduct</em>. British Computer Society.
            </li>
            <li>
              Correa, N.K., Galvão, C., Santos, J.W. et al. (2023) ‘Worldwide AI ethics: A review of 200
              guidelines and recommendations for AI governance’, <em>Patterns</em>, 4(10), 100857.
            </li>
            <li>
              Deckard, B. (2023) ‘Executive considerations for adopting generative AI responsibly’.
              Industry white paper.
            </li>
            <li>
              European Parliament and Council (2016) <em>General Data Protection Regulation (EU) 2016/679</em>.
              Available at: https://gdpr.eu/ (Accessed: 12 November 2025).
            </li>
          </ul>
        </div>

        <!-- Reflective Activity 2 (placeholder) -->
        <div class="card">
          <div class="card-header">
            <h2>Reflective Activity 2: Inappropriate Use of Surveys</h2>
            <div class="meta small-muted">
              Unit 5 · Research ethics and survey design
            </div>
          </div>
          <p>
            [Insert your reflection on the case study about inappropriate use of surveys, including what you
            learned about consent, bias, and good practice in data collection.]
          </p>
        </div>

        <!-- Reasoning Quiz mini-reflection (placeholder) -->
        <div class="card">
          <div class="card-header">
            <h2>Reasoning Quiz – Mini Reflection</h2>
            <div class="meta small-muted">
              Unit 1 · Logical reasoning and problem-solving
            </div>
          </div>
          <p>
            [Briefly describe how you found the reasoning quiz, any areas you found challenging, and how this links to
            later units involving logic, statistics or structured thinking.]
          </p>
        </div>

        <!-- Final 1,000-word Reflection (placeholder) -->
        <div class="card">
          <div class="card-header">
            <h2>Final 1,000-word Reflection (Unit 12)</h2>
            <div class="meta small-muted">
              Unit 12 · Overall reflection on the module
            </div>
          </div>
          <p>
            [Insert full final reflection of approximately 1,000 words, covering what you have learned about computing,
            statistics, research, ethics, and your development as an MSc AI student.]
          </p>
        </div>
      </section>

      <aside class="sidebar">
        <div class="card">
          <div class="card-header">
            <h3>Reflective themes</h3>
          </div>
          <ul class="clean">
            <li><span class="tag-pill">Ethics</span>Responsible use of computing and data</li>
            <li><span class="tag-pill">Research</span>Designing fair and ethical studies</li>
            <li><span class="tag-pill">Skills</span>Growth in analysis, writing and critical thinking</li>
            <li><span class="tag-pill">Future</span>How this module feeds into later MSc AI modules</li>
          </ul>
        </div>
      </aside>
    </main>

    <footer class="site-footer">
      Reflections · Research Methods and Professional Practice – e-Portfolio
    </footer>
  </div>
</body>
</html>


