<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Reflective Activity 1 – Ethics in Computing & Generative AI</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #f7f3ec;
      --card: #ffffff;
      --accent: #1f2937;   /* dark text */
      --accent-soft: #b39572; /* beige accent */
      --muted: #6b7280;
      --border: #e5ded0;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
        Roboto, sans-serif;
      background: var(--bg);
      color: var(--accent);
      line-height: 1.6;
    }

    a {
      color: #1d4ed8;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    header {
      background: #111827;
      color: #f9fafb;
      padding: 1.5rem 1.75rem;
      border-bottom: 4px solid var(--accent-soft);
    }

    header h1 {
      margin: 0 0 0.25rem 0;
      font-size: 1.6rem;
      letter-spacing: 0.03em;
    }

    header .meta {
      font-size: 0.9rem;
      color: #e5e7eb;
    }

    nav {
      background: #111827;
      border-top: 1px solid rgba(249, 250, 251, 0.08);
      border-bottom: 1px solid var(--border);
      padding: 0.5rem 1.75rem 0.8rem;
    }

    nav a {
      color: #e5e7eb;
      font-size: 0.9rem;
      margin-right: 1.2rem;
      position: relative;
      padding-bottom: 0.15rem;
    }

    nav a.active {
      font-weight: 600;
      color: #fbbf77;
    }

    nav a.active::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: -0.15rem;
      width: 100%;
      height: 2px;
      background: #fbbf77;
      border-radius: 999px;
    }

    main {
      max-width: 900px;
      margin: 2rem auto 3rem;
      padding: 0 1.25rem 3rem;
    }

    .card {
      background: var(--card);
      border-radius: 14px;
      border: 1px solid var(--border);
      box-shadow: 0 18px 45px rgba(15, 23, 42, 0.06);
      padding: 1.8rem 1.75rem 2rem;
    }

    h2 {
      margin-top: 0;
      font-size: 1.5rem;
    }

    .badge {
      display: inline-flex;
      align-items: center;
      gap: 0.4rem;
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--muted);
      margin-bottom: 0.5rem;
    }

    .badge span {
      width: 8px;
      height: 8px;
      border-radius: 999px;
      background: var(--accent-soft);
    }

    .meta-line {
      font-size: 0.9rem;
      color: var(--muted);
      margin-bottom: 1.5rem;
    }

    h3 {
      margin-top: 1.4rem;
      font-size: 1.15rem;
    }

    ul {
      padding-left: 1.3rem;
    }

    li {
      margin-bottom: 0.3rem;
    }

    .references {
      margin-top: 1.6rem;
      padding-top: 1.2rem;
      border-top: 1px dashed var(--border);
    }

    .references h3 {
      margin-top: 0;
    }

    .references p {
      font-size: 0.92rem;
      margin-bottom: 0.35rem;
    }

    @media (max-width: 640px) {
      header,
      nav {
        padding-left: 1rem;
        padding-right: 1rem;
      }

      main {
        padding: 0 1rem 2.5rem;
      }

      .card {
        padding: 1.4rem 1.3rem 1.6rem;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>Research Methods &amp; Professional Practice – e-Portfolio</h1>
    <div class="meta">
      Student: Naeema Abdalla Ahmed Alnaqbi &nbsp;|&nbsp;
      Programme: MSc Artificial Intelligence
    </div>
  </header>

  <nav>
    <a href="index.html">Home</a>
    <a href="discussion.html">Discussion 1 (Units 1–3)</a>
    <a href="reflective-activity-1.html" class="active">Reflective Activity 1</a>
    <a href="stats.html">Statistics Exercises</a>
    <a href="proposal-eval.html">Lit Review &amp; Proposal</a>
    <a href="reflection.html">Final 1,000-word Reflection</a>
  </nav>

  <main>
    <section class="card">
      <div class="badge">
        <span></span>
        Reflective Activity 1 – Ethics in Computing in the Age of Generative AI
      </div>
      <h2>Ethics in Computing in the Age of Generative AI</h2>
      <p class="meta-line">
        Author: <strong>Naeema Abdalla Ahmed Alnaqbi</strong> &nbsp;|&nbsp;
        Word count: ~1,000
      </p>

      <h3>What – Context and focus</h3>
      <p>
        Since late 2022, generative AI (GenAI) tools have moved from research
        labs into everyday life. They now write code, draft emails, summarise
        medical articles and generate realistic images. As Correa et al. (2023)
        show, this rapid adoption has triggered a wave of more than 200 AI
        ethics guidelines worldwide, but these documents differ in scope,
        terminology and legal force. Deckard (2023) adds that organisations face
        strong pressure to “move fast” with GenAI to gain competitive
        advantage, even when governance structures are still immature.
      </p>
      <p>
        For computing professionals, this raises a simple but difficult
        question: how can we use GenAI responsibly when global norms are
        fragmented and the technology is still evolving? In this reflection I
        draw on Correa et al. (2023), Deckard (2023), and professional codes
        such as ACM (2018) and BCS (2021, 2024) to develop my own view. I also
        connect the discussion to my background as a biomedical engineer and AI
        student who is particularly interested in healthcare and human
        physiology.
      </p>

      <h3>So what – Analysis and position</h3>

      <h4>1. Fragmented principles and the need for enforceable baselines</h4>
      <p>
        Correa et al. (2023) show that most AI ethics documents repeat similar
        high-level values – fairness, transparency, accountability, privacy,
        human oversight – but differ on how to implement or enforce them. Many
        guidelines are voluntary and lack mechanisms for monitoring or
        sanctions. I see a risk that GenAI becomes governed by “ethics washing”:
        beautiful principles with little impact on practice.
      </p>
      <p>
        From my perspective, a more convincing approach is layered. Global and
        regional frameworks (e.g. OECD principles, EU AI Act, GDPR) should
        define minimum, enforceable baselines for safety, transparency and data
        protection. Professional bodies such as ACM and BCS should translate
        these into expectations for individual conduct (competence, integrity,
        duty to the public). Finally, organisations must convert all of this
        into concrete policies, risk registers and technical controls inside
        their software development life cycle.
      </p>

      <h4>2. Risk-tiered governance rather than one-size-fits-all rules</h4>
      <p>
        Not every GenAI system has the same potential for harm. A brainstorming
        assistant for marketing copy is very different from a model that helps
        clinicians prioritise patients or predicts mental-health risk from text.
        I agree with Correa et al. (2023) that ethical guidance should be
        sensitive to context. In practice I support a risk-tiered model:
        low-risk applications may rely on lightweight safeguards, whereas
        high-risk uses require formal impact assessments, red-teaming, domain
        expert review, and ongoing monitoring.
      </p>
      <p>
        This approach matches my own experience in healthcare technology, where
        risk classification already determines the depth of testing and
        documentation. Extending this logic to GenAI would link ethical
        concerns to concrete engineering practices such as evaluation datasets,
        fail-safe design and incident reporting.
      </p>

      <h4>3. Data governance, privacy and security</h4>
      <p>
        Many GenAI harms begin in the data pipeline rather than in the model
        itself. Training on scraped data without clear consent or licensing
        raises questions under data protection and intellectual property law.
        Using patient information with public cloud models can directly violate
        confidentiality rules. As someone working in the health sector, I find
        this aspect particularly sensitive.
      </p>
      <p>
        I believe responsible GenAI requires strong data governance: clear legal
        bases for processing, purpose limitation, data minimisation, and robust
        security controls. Where models are trained or fine-tuned on personal
        data, organisations should conduct Data Protection Impact Assessments
        and ensure that outputs cannot easily re-identify individuals. This
        links directly to GDPR requirements and to ACM principles 1.2 (avoid
        harm) and 1.6 (respect privacy).
      </p>

      <h4>4. Professional competence and honest use of GenAI</h4>
      <p>
        Correa et al. (2023) highlight that many guidelines are aspirational,
        but Deckard (2023) stresses that successful GenAI adoption depends on
        real organisational capability. For individual computing professionals,
        competence now includes understanding GenAI’s limitations:
        hallucinations, bias, prompt injection, misuse of training data, and the
        need for human oversight.
      </p>
      <p>
        In my own studies I already rely on GenAI tools to brainstorm ideas and
        polish writing, but this activity made me think more critically about
        transparency and academic integrity. I must clearly distinguish between
        my own analysis and AI-generated suggestions, and always acknowledge
        sources correctly. Professionally, I see a similar responsibility:
        engineers should document how GenAI was used in design or documentation
        and avoid presenting AI output as unquestionable truth.
      </p>

      <h4>5. Human and physiological perspective</h4>
      <p>
        As a biomedical engineer, I cannot ignore the human body behind the
        data. Physiological signals, clinical notes and mental-health
        assessments are deeply personal. If GenAI is used to analyse such data –
        for example predicting deterioration in an ambulance or generating
        patient information leaflets – errors can harm both physical and
        emotional wellbeing.
      </p>
      <p>
        My long-term goal is to integrate AI with healthcare in ways that
        support clinicians rather than replace them. This reflection reinforced
        the importance of human-centred design: interfaces that are
        understandable to patients, explanations that match their emotional
        state, and workflows that keep clinicians in control. GenAI can be a
        powerful assistant, but only if we design around human limits, including
        cognitive overload and stress.
      </p>
      <p>
        <strong>My position:</strong> Generative AI should be governed through
        risk-tiered, enforceable frameworks that combine global principles,
        local law, professional codes and concrete engineering practices. Data
        governance and human-centred design are as important as model
        performance.
      </p>

      <h3>Now what – Recommended course of action and impacts</h3>
      <p>If I had to recommend a practical course of action, it would include:</p>
      <ul>
        <li>
          Adopting clear GenAI policies and risk registers in organisations,
          linking each use case to purpose, lawful basis, data categories and
          risk tier.
        </li>
        <li>
          Requiring impact assessments and red-team testing for medium and
          high-risk applications, especially in health, education, employment
          and policing.
        </li>
        <li>
          Mandating transparency mechanisms such as model cards, change logs,
          and clear labelling of AI-generated content.
        </li>
        <li>
          Strengthening professional education so that computing practitioners
          understand GenAI failure modes, legal obligations and ethical
          reasoning, not just technical skills.
        </li>
        <li>
          Including diverse stakeholders – patients, disabled users, marginalised
          groups – in the design and evaluation of GenAI systems.
        </li>
      </ul>
      <p>
        Legally, these steps help organisations demonstrate compliance with data
        protection and consumer protection laws and with sector-specific
        regulations in healthcare. Socially, they can reduce bias, improve
        accessibility and preserve public trust. Professionally, they align with
        ACM and BCS duties to act in the public interest, maintain competence,
        and be honest about system limitations.
      </p>

      <h3>Conclusion</h3>
      <p>
        This activity moved my thinking from “GenAI is exciting and powerful” to
        a more nuanced view: GenAI is also fragile, highly context-dependent and
        deeply entangled with law, society and human bodies. Reading Correa et
        al. (2023) made me appreciate how fragmented global AI governance
        currently is, while Deckard (2023) emphasised that uncritical adoption
        is strategically risky.
      </p>
      <p>
        As I continue my MSc and future work in AI-enabled healthcare, I want to
        be the kind of professional who can translate ethical principles into
        practical decisions: asking “who might be harmed?”, “what data are we
        really using?” and “how can we design this so a stressed clinician or
        patient can still understand it?”. For me, that is what ethics in the
        age of generative AI truly means.
      </p>

      <div class="references">
        <h3>References</h3>
        <p>
          ACM (2018) <em>ACM Code of Ethics and Professional Conduct</em>.
          Available at: https://www.acm.org/code-of-ethics (Accessed: 12
          November 2025).
        </p>
        <p>
          BCS (2021) <em>Code of Conduct</em>. British Computer Society.
          Available at:
          https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/
          (Accessed: 12 November 2025).
        </p>
        <p>
          BCS (2024) <em>BCS Code of Conduct</em>. British Computer Society.
        </p>
        <p>
          Correa, N.K., Galvão, C., Santos, J.W. <em>et al.</em> (2023)
          ‘Worldwide AI ethics: A review of 200 guidelines and recommendations
          for AI governance’, <em>Patterns</em>, 4(10), 100857.
        </p>
        <p>
          Deckard, B. (2023) ‘Executive considerations for adopting generative
          AI responsibly’. Industry white paper.
        </p>
        <p>
          European Parliament and Council (2016) <em>General Data Protection
          Regulation (EU) 2016/679</em>. Available at: https://gdpr.eu/
          (Accessed: 12 November 2025).
        </p>
      </div>
    </section>
  </main>
</body>
</html>
