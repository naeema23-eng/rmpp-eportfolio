<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Discussions – Research Methods and Professional Practice e-Portfolio</title>
  <link rel="stylesheet" href="style.css">
</head>

<body id="top">
  <div class="main-wrapper">

    <header class="site-header">
      <div class="brand">
        <h1>Research Methods and Professional Practice</h1>
        <span>MSc Artificial Intelligence – e-Portfolio</span>
      </div>

      <nav class="main-nav">
        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="discussion.html" class="active">Discussions</a></li>
          <li><a href="research.html">Research &amp; Proposal</a></li>
          <li><a href="statistics.html">Statistics &amp; Data</a></li>
          <li><a href="reflections.html">Reflections</a></li>
          <li><a href="activities.html">Activities</a></li>
          <li><a href="about.html">About</a></li>
        </ul>
      </nav>
    </header>

    <main class="page-grid">
      <section>

        <!-- =====================================================
             Collaborative Discussion 1
        ====================================================== -->
        <div class="card" id="cd1">
          <div class="card-header">
            <h2 class="page-title">Collaborative Discussion 1: Codes of Ethics &amp; Professional Conduct</h2>
            <div class="meta small-muted">
              Units 1–3 · ACM &amp; BCS Codes · Case: Data Breach at a Medical Database
            </div>
          </div>

          <h3>Initial Post – Data Breach at a Medical Database</h3>

          <p>
            A hospital’s database breach is not only a security failure but an ethical collapse with legal, social, and
            professional consequences—especially when downstream AI systems may learn from, or be audited against,
            compromised data. Under the ACM Code of Ethics (2018), the organisation and its computing professionals
            failed to avoid harm (1.2) and respect privacy (1.6) due to inadequate safeguards and delayed disclosure.
            Principle 2.5 also requires thorough evaluation of risks and impacts, including robust pre-breach assessment
            and transparent post-breach response.
          </p>

          <p>
            The BCS Code of Conduct (2021) similarly requires acting in the public interest and upholding privacy, security,
            and wellbeing (Clause 1a), while Clause 2 emphasises competence and integrity. Research shows that health-data
            breaches amplify social harms such as re-identification, stigma, or discriminatory decision-making (Shen and Ma,
            2022). If leaked datasets are reused in AI training, they may embed structural bias or distort predictions
            (Mehrabi et al., 2021), contradicting ACM 1.4 (fairness) and BCS non-discrimination expectations.
          </p>

          <p>
            Legally, GDPR Articles 33–34 mandate timely breach notification to authorities and affected individuals.
            Violating these duties reflects weak professional governance. As Soomro, Shah and Ahmed (2020) argue, many
            breaches arise from organisational deficiencies—poor controls, weak accountability, and insufficient risk
            management—not technical errors alone.
          </p>

          <p>
            Following module guidance, this case demonstrates the chain:
            <em>Professional lapse → Legal breach → Social harm.</em> In healthcare, this chain is intensified because
            trust is essential for safe digital health and ethical AI innovation.
          </p>

          <p>
            To align with ACM/BCS expectations, organisations should adopt:
          </p>
          <ul>
            <li>Privacy-by-design and security-by-design approaches</li>
            <li>Tested breach-response procedures</li>
            <li>Independent audits and bias/impact assessments before AI reuse of clinical data</li>
            <li>Transparent communication to rebuild public and patient trust</li>
          </ul>

          <h4>References</h4>
          <ul>
            <li>ACM (2018) <em>ACM Code of Ethics and Professional Conduct</em>.</li>
            <li>BCS (2021) <em>Code of Conduct</em>. British Computer Society.</li>
            <li>Corrêa, N.K. et al. (2023) ‘Worldwide AI ethics: A review of 200 guidelines’, <em>Patterns</em>, 4(10), 100857.</li>
            <li>European Parliament and Council (2016) <em>General Data Protection Regulation (EU) 2016/679</em>.</li>
            <li>Mehrabi, N. et al. (2021) ‘A survey on bias and fairness in machine learning’, <em>ACM Computing Surveys</em>, 54(6).</li>
            <li>Shen, N. and Ma, J. (2022) ‘Ethical challenges in protecting health information in the digital era’, <em>Journal of Medical Systems</em>, 46(3).</li>
            <li>Soomro, Z.A., Shah, M.H. and Ahmed, J. (2020) ‘Information security management needs more holistic approaches’, <em>International Journal of Information Management</em>, 51.</li>
          </ul>

          <hr>

          <h3>Peer Response 1 – Reply to Nikolaos: Data Poisoning and Accountability by Design</h3>

          <p>
            Hi Nikolaos, excellent analysis. I strongly agree that preserving a corrupted model after evidence of targeted
            manipulation breaches ACM 1.2 (avoid harm) and BCS Public Interest responsibilities. What you describe reflects
            a classic case of data poisoning within a human-in-the-loop feedback loop. Beyond bias mitigation, professional
            competence (ACM 2.2; BCS Clause 2) requires engineering adversarial resilience—such as gated feedback channels,
            anomaly detection on label patterns, and rollback procedures when poisoning or drift is detected.
          </p>

          <p>
            Your transparency point can also be framed as accountability by design: documenting known limitations, publishing
            model cards or change logs, and enabling stakeholders (e.g., parents, schools, civil-society organisations) to
            contest errors. This aligns with the EU Trustworthy AI guidelines emphasising human oversight and traceability.
          </p>

          <p>
            Fairness also needs measurable thresholds. Monitoring false-positive rates across protected groups and enforcing
            go/no-go deployment criteria strengthens both ACM 1.4 and BCS expectations around non-discrimination. When
            thresholds fail, ethics and engineering teams should co-sign remediation steps before redeployment.
          </p>

          <p>
            A final question for you: should independent red-team audits be mandatory before content-moderation systems for
            minors are deployed—and should this be enforced by ACM/BCS disciplinary bodies or by statutory law?
          </p>

          <h4>References</h4>
          <ul>
            <li>ACM (2018) <em>ACM Code of Ethics and Professional Conduct</em>.</li>
            <li>BCS (2021) <em>Code of Conduct</em>. British Computer Society.</li>
            <li>European Commission High-Level Expert Group on AI (2019) <em>Ethics Guidelines for Trustworthy AI</em>.</li>
            <li>Fjeld, J. et al. (2020) <em>Principled Artificial Intelligence</em>. Berkman Klein Center.</li>
          </ul>

          <hr>

          <h3>Peer Response 2 – Reply to Nelson: Accessibility as Governance</h3>

          <p>
            Hi Nelson, strong and clear analysis. You rightly show that releasing an inaccessible feature violates ACM
            principles 1.1, 1.2, 1.3 and 1.4, as well as the BCS duty to act in the public interest and prevent
            discrimination. Your point regarding the Equality Act 2010 is crucial: accessibility is not optional, and
            neglecting it constitutes unlawful discrimination.
          </p>

          <p>
            To extend your argument, research shows that accessibility failures often arise from organisational governance
            gaps, not just scheduling pressures. El Morr et al. (2024) find that many developers lack accessibility training,
            meaning violations are not identified early. This reflects a structural ethical issue: failing to meet ACM 2.3
            (respect rules) and BCS Clause 2 (competence) because teams simply do not have embedded accessibility practices.
          </p>

          <p>
            Furthermore, accessibility maturity research (Lazar, Goldstein and Taylor, 2017) shows that embedding
            accessibility from the start significantly reduces cost, risk, and harm, while improving innovation. This
            positions accessibility as a core ethical requirement, not a late technical fix.
          </p>

          <p>
            Your post effectively connects ethical, social, and legal consequences. Reinforcing accessibility as a governance
            requirement would protect users and strengthen professional integrity across the softwar
